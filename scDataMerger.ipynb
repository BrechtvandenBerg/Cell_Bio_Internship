{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate the base directory of the data\n",
    "base_dir = \"/scratch/bvdberg/SoloTE/\"\n",
    "\n",
    "# Use glob to find directories matching the pattern\n",
    "directories = glob.glob(\n",
    "    os.path.join(base_dir, \"run_*/\", \"*_SoloTE_output/\", \"*_locustes_MATRIX\")\n",
    ")\n",
    "anndata = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in directories:\n",
    "    # Set the wildcard per directory\n",
    "    wildcard = os.path.basename(directory)\n",
    "    identifier = str(wildcard.split(\"_locustes_MATRIX\")[0])\n",
    "    \n",
    "    # Skip directories with an empty identifier\n",
    "    if not identifier:\n",
    "        continue\n",
    "\n",
    "    # Check if the required files exist in the directory\n",
    "    matrix_file = os.path.join(directory, \"matrix.mtx\")\n",
    "    barcodes_file = os.path.join(directory, \"barcodes.tsv\")\n",
    "    features_file = os.path.join(directory, \"features.tsv\")\n",
    "\n",
    "    if (\n",
    "        os.path.exists(matrix_file)\n",
    "        and os.path.exists(barcodes_file)\n",
    "        and os.path.exists(features_file)\n",
    "    ):\n",
    "        # Create a new AnnData object for each directory\n",
    "        adata = sc.AnnData()\n",
    "\n",
    "        # Read the matrix using scanpy\n",
    "        adata = sc.read_mtx(matrix_file)\n",
    "        adata = adata.transpose()\n",
    "\n",
    "        # Read barcodes and features using pandas\n",
    "        barcodes = pd.read_csv(barcodes_file, sep=\"\\t\", header=None, names=[\"barcode\"])\n",
    "        features = pd.read_csv(features_file, sep=\"\\t\", header=None, names=[\"gene_name\"])\n",
    "\n",
    "        # Set obs_names and var_names\n",
    "        adata.obs_names = barcodes[\"barcode\"]\n",
    "        adata.var_names = features[\"gene_name\"]\n",
    "\n",
    "        anndata[identifier] = adata\n",
    "\n",
    "    else:\n",
    "        print(f\"Required files not found in directory: {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the adata sets to one data set with the Concatenate function, we use 'outer' to preserve as much data as possible. Missing variablles will become NaN values\n",
    "combined_adata = ad.concat(anndata, label=\"dataset_origin\", join=\"outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the data to make it comparable to a whitelisted h5ad formatted file, by manipulating the 'dataset origin' column\n",
    "combined_adata.obs.index = combined_adata.obs['dataset_origin'].str[1:] + '_' + combined_adata.obs.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_adata.write_h5ad(\"combined_bonemarrow_sets.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist_anndata = ad.read_h5ad(\"bonemarrow_m.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the anndata based on the whitelist\n",
    "filtered_anndata = combined_adata[combined_adata.obs.index.isin(whitelist_anndata.obs.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy metadata from the whitelist to the filtered dataset\n",
    "filtered_anndata.obs = whitelist_anndata.obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the 'TE_type' column\n",
    "filtered_anndata.var['te_type'] = 'GENE'\n",
    "\n",
    "# Create a mask based on the structure you want to match\n",
    "mask = filtered_anndata.var_names.str.count(':') > 1\n",
    "\n",
    "# Apply the structure-based modification to matching entries\n",
    "matching_entries = filtered_anndata.var_names[mask]\n",
    "matching_entries = matching_entries.astype(str).str.split(':').str[2]\n",
    "matching_entries = matching_entries.str.split('|').str[0]\n",
    "\n",
    "# Update the 'te_type' column for matching entries\n",
    "filtered_anndata.var['te_type'][mask] = matching_entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_anndata.write_h5ad(\"whitelisted_bonemarrow.h5ad\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy",
   "language": "python",
   "name": "scanpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
